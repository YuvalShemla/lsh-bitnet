# LSH-BitNet: Locality-Sensitive Hashing Attention for BitNet Models

A modular framework for experimenting with LSH-based attention mechanisms on BitNet models, enabling efficient attention computation through locality-sensitive hashing.

## Overview

This repository provides a flexible framework to:
- Load BitNet models from HuggingFace
- Replace standard attention with LSH-based attention variants
- Experiment with different hashing strategies (bit-sampling, SimHash, etc.)
- Evaluate performance on various language modeling datasets
- Compare efficiency (attention comparisons) vs accuracy trade-offs

---

## Repository Structure

```
lsh-bitnet/
‚îú‚îÄ‚îÄ src/                           # Main source code
‚îÇ   ‚îú‚îÄ‚îÄ config.py                  # Configuration dataclasses
‚îÇ   ‚îú‚îÄ‚îÄ attention/                 # Attention backends
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ wrapper.py            # AttentionWrapper (main integration)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ full_attention.py    # Baseline full attention
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ base_lsh.py           # LSH attention base class
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ bit_sampling_lsh.py   # Bit-sampling LSH attention
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ simhash_lsh.py         # SimHash LSH attention (future)
‚îÇ   ‚îú‚îÄ‚îÄ hash_functions/           # Hashing strategies
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ base.py               # HashingStrategy interface
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ bit_sampling.py       # Bit-sampling hasher
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ simhash.py            # SimHash hasher (future)
‚îÇ   ‚îú‚îÄ‚îÄ indexing/                 # Indexing functions (future)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ data_interface/           # Dataset loaders
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ enwik8.py             # EnWik8 dataset
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ wikitext103.py        # WikiText-103 dataset
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ synthetic_ny_adj.py   # Synthetic New York adjective task
‚îÇ   ‚îú‚îÄ‚îÄ evaluation/                # Evaluation metrics
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ lm_metrics.py         # Perplexity evaluation
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ synthetic_metrics.py  # Accuracy evaluation for synthetic tasks
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ comparison_logger.py  # Comparison tracking
‚îÇ   ‚îú‚îÄ‚îÄ model_adapters/            # Model loading & adapters
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ adapters.py           # Model adapter pattern
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ registry.py           # Model registry
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ bitnet_hf.py          # BitNet-specific utilities
‚îÇ   ‚îî‚îÄ‚îÄ utils/                     # Utilities
‚îÇ       ‚îú‚îÄ‚îÄ logging.py
‚îÇ       ‚îî‚îÄ‚îÄ seed.py
‚îú‚îÄ‚îÄ scripts/                       # Evaluation scripts
‚îÇ   ‚îú‚îÄ‚îÄ bitnet_baseline_wikitext103.py  # WikiText-103 perplexity baseline
‚îÇ   ‚îî‚îÄ‚îÄ eval_synthetic_ny_adj.py        # Synthetic task accuracy baseline
‚îú‚îÄ‚îÄ configs/                       # Experiment configurations
‚îÇ   ‚îú‚îÄ‚îÄ wt2_bitnet_full.yaml
‚îÇ   ‚îú‚îÄ‚îÄ wt2_bitnet_lsh_bitsampling.yaml
‚îÇ   ‚îî‚îÄ‚îÄ enwik8_bitnet_lsh_bitsampling.yaml
‚îú‚îÄ‚îÄ models/                        # HuggingFace model cache (data)
‚îÇ   ‚îî‚îÄ‚îÄ models--microsoft--bitnet-b1.58-2B-4T/
‚îú‚îÄ‚îÄ data/                          # Dataset cache
‚îÇ   ‚îú‚îÄ‚îÄ enwik8                     # EnWik8 raw data
‚îÇ   ‚îî‚îÄ‚îÄ wikitext103/               # WikiText-103 cache
‚îú‚îÄ‚îÄ results/                       # Evaluation results
‚îÇ   ‚îú‚îÄ‚îÄ baseline_wikitext103_*.json
‚îÇ   ‚îî‚îÄ‚îÄ baseline_synthetic_ny_adj_*.json
‚îî‚îÄ‚îÄ tests/                         # Unit tests
```

---

## Current Status

### ‚úÖ Baseline Evaluations Completed

We have established baseline performance on two dataset types:

#### 1. **WikiText-103 Perplexity Baseline**
- **Dataset**: WikiText-103 validation split
- **Evaluation**: Language modeling perplexity
- **Coverage**: 252,672 tokens (full validation set)
- **Configuration**: 987 batches √ó 2 sequences √ó 128 tokens
- **Result**: **Perplexity = 33.12**
- **File**: `results/baseline_wikitext103_validation_all_all_*.json`

#### 2. **Synthetic New York Adjective Task Baseline**
- **Dataset**: Synthetic task requiring tracking 2nd mention of "New York"
- **Evaluation**: Accuracy (extracting correct adjective)
- **Samples**: 50 examples
- **Result**: **Accuracy = 40.0%** (20/50 correct)
- **File**: `results/baseline_synthetic_ny_adj_50samples_*.json`

**Note**: The synthetic task accuracy is low because the model is not fine-tuned for this specific task. This baseline will serve as a comparison point for LSH attention implementations.

### üìä Datasets

#### WikiText-103
- **Source**: HuggingFace `Salesforce/wikitext` (wikitext-103-raw-v1)
- **Type**: Wikipedia articles (raw text)
- **Size**: 
  - Train: 1,801,350 examples
  - Validation: 3,760 examples (~252,725 tokens)
  - Test: 4,358 examples
- **Purpose**: Standard language modeling benchmark
- **Evaluation**: Perplexity (lower is better)

#### Synthetic New York Adjective Task (toy example)
- **Type**: Synthetically generated examples
- **Task**: Extract the adjective before the Nth mention of "New York"
- **Purpose**: Test attention mechanisms (requires long-range dependency tracking)
- **Evaluation**: Accuracy (higher is better)
- **Why useful**: 
  - Explicitly tests attention capabilities
  - Easy to control difficulty (distance between mentions)
  - Interpretable results

#### Future Datasets
We plan to expand to additional datasets in future steps:
- WikiText-2 (smaller, faster evaluation)
- EnWik8 (character-level, different tokenization)
- Other language modeling benchmarks

---

## BitNet Model Architecture

### Model: `microsoft/bitnet-b1.58-2B-4T`

#### Top-Level Architecture

```
BitNetForCausalLM
‚îî‚îÄ‚îÄ BitNetModel (attribute: 'model')
    ‚îú‚îÄ‚îÄ embed_tokens: Embedding layer
    ‚îú‚îÄ‚îÄ layers: ModuleList (30 decoder layers)
    ‚îî‚îÄ‚îÄ norm: BitNetRMSNorm (final layer norm)
```

#### Model Configuration

| Parameter | Value | Description |
|-----------|-------|-------------|
| **Hidden Size** | 2560 | Model dimension (d_model) |
| **Number of Layers** | 30 | Transformer decoder layers |
| **Attention Heads** | 20 | Query attention heads |
| **Key-Value Heads** | 5 | Key/Value attention heads (GQA) |
| **Head Dimension** | 128 | Dimension per attention head |
| **Intermediate Size** | 6912 | FFN hidden dimension |
| **Max Position Embeddings** | 4096 | Maximum sequence length |
| **Vocab Size** | 128256 | Vocabulary size |
| **Activation** | relu2 | ReLU¬≤ activation function |
| **RMS Norm Eps** | 1e-05 | Layer normalization epsilon |

**Key Feature: Grouped Query Attention (GQA)**
- Query projection: 20 heads √ó 128 dims = 2560 dims
- Key/Value projections: 5 heads √ó 128 dims = 640 dims each
- GQA ratio: 4:1 (each KV head serves 4 Q heads)

#### Decoder Layer Structure

Each `BitNetDecoderLayer` contains:

```
BitNetDecoderLayer
‚îú‚îÄ‚îÄ input_layernorm: BitNetRMSNorm
‚îú‚îÄ‚îÄ self_attn: BitNetAttention  ‚¨ÖÔ∏è TARGET FOR LSH WRAPPER
‚îÇ   ‚îú‚îÄ‚îÄ q_proj: AutoBitLinear (2560 ‚Üí 2560)  [20 heads]
‚îÇ   ‚îú‚îÄ‚îÄ k_proj: AutoBitLinear (2560 ‚Üí 640)   [5 heads]
‚îÇ   ‚îú‚îÄ‚îÄ v_proj: AutoBitLinear (2560 ‚Üí 640)   [5 heads]
‚îÇ   ‚îú‚îÄ‚îÄ o_proj: AutoBitLinear (2560 ‚Üí 2560)  [output projection]
‚îÇ   ‚îî‚îÄ‚îÄ attn_sub_norm: BitNetRMSNorm
‚îú‚îÄ‚îÄ post_attention_layernorm: BitNetRMSNorm
‚îî‚îÄ‚îÄ mlp: BitNetMLP
    ‚îú‚îÄ‚îÄ gate_proj: AutoBitLinear
    ‚îú‚îÄ‚îÄ up_proj: AutoBitLinear
    ‚îú‚îÄ‚îÄ down_proj: AutoBitLinear
    ‚îî‚îÄ‚îÄ ffn_sub_norm: BitNetRMSNorm
```

#### Attention Module Details

**Class:** `BitNetAttention`

**Forward Signature:**
```python
forward(
    hidden_states: torch.Tensor,                    # [B, T, 2560]
    position_embeddings: tuple,                      # RoPE embeddings
    attention_mask: Optional[torch.Tensor] = None,
    past_key_values: Optional[Cache] = None,
    cache_position: Optional[torch.LongTensor] = None,
    **kwargs
) -> tuple
```

**Tensor Shapes:**
- Input: `[batch_size, seq_len, 2560]`
- Q projection output: `[batch_size, seq_len, 2560]` ‚Üí reshaped to `[B, 20, T, 128]`
- K projection output: `[batch_size, seq_len, 640]` ‚Üí reshaped to `[B, 5, T, 128]`
- V projection output: `[batch_size, seq_len, 640]` ‚Üí reshaped to `[B, 5, T, 128]`

**Attention Computation (Standard):**
1. Q, K, V are projected from hidden states
2. Q is split into 20 heads, K/V into 5 heads each
3. For GQA, each KV head is broadcast to 4 Q heads
4. Attention scores computed: `Q @ K^T / sqrt(128)`
5. Causal masking applied (lower triangular)
6. Softmax and attention-weighted sum: `softmax(scores) @ V`
7. Output projection: `[B, T, 2560]`

**Full Attention Complexity**: O(T¬≤) comparisons per head

---

## Implementation Plan

### Phase 1: Simple Bit-Sampling LSH Attention (Current Focus)

**Goal**: Implement basic LSH-based attention using bit-sampling to partition tokens into buckets.

#### Approach:
1. **Attention Wrapper**: Replace `BitNetAttention` modules with `AttentionWrapper`
   - Wrapper reuses Q/K/V/O projections from original module
   - Intercepts attention computation
   - Delegates to LSH backend when enabled

2. **Bit-Sampling LSH**:
   - Binarize Q, K tensors (threshold at 0.0)
   - Sample random bit positions for hashing
   - Compute hash buckets for each token position
   - Group queries and keys by bucket ID
   - Compute attention only within matching buckets

3. **Metrics to Track**:
   - **Accuracy**: Perplexity on WikiText-103, accuracy on synthetic task
   - **Efficiency**: Number of query-key comparisons made
   - **Comparison Ratio**: LSH comparisons / Full attention comparisons

#### Expected Outcomes:
- Reduced attention comparisons (from O(T¬≤) to O(T¬∑B) where B << T)
- Some accuracy degradation (to be measured)
- Clear efficiency vs accuracy trade-off curve

### Phase 2: LSH Forest for Dynamic Candidate Selection

**Goal**: Implement LSH Forest to allow dynamic candidate selection without pre-specifying number of buckets.

#### Approach:
1. **Multiple Hash Tables**: Use multiple independent hash functions
2. **Union of Matches**: For each query, find matching keys across all tables
3. **Dynamic Bucket Selection**: Automatically determine relevant buckets per query
4. **Adaptive Thresholding**: Adjust candidate set size based on query characteristics

#### Advantages over Simple Bucketing:
- No need to pre-specify bucket count
- Better recall (finds more relevant keys)
- More robust to hash collisions
- Can adapt to different query types

### Phase 3: Comprehensive Comparison

**Evaluation Framework**:
1. **Full Attention** (baseline)
   - Complexity: O(T¬≤) comparisons
   - Accuracy: Baseline (current results)

2. **Bucket Attention** (Phase 1)
   - Complexity: O(T¬∑B) comparisons (B = average bucket size)
   - Accuracy: To be measured
   - Trade-off: Speedup vs accuracy loss

3. **LSH Forest Attention** (Phase 2)
   - Complexity: O(T¬∑C) comparisons (C = dynamic candidate set)
   - Accuracy: To be measured
   - Trade-off: Better accuracy than buckets, more comparisons

**Metrics to Compare**:
- **Perplexity** on WikiText-103
- **Accuracy** on synthetic task
- **Number of comparisons** per forward pass
- **Speedup factor** vs full attention
- **Memory usage** (if applicable)

---

## Usage

### Running Baseline Evaluations

**WikiText-103 Perplexity:**
```bash
python scripts/bitnet_baseline_wikitext103.py \
    --max-batches 500 \
    --batch-size 2 \
    --seq-len 128 \
    --split validation \
    --device cpu
```

**Synthetic Task:**
```bash
python scripts/eval_synthetic_ny_adj.py \
    --n-samples 50 \
    --batch-size 1 \
    --device cpu
```

### Results

Results are automatically saved to `results/` folder with descriptive filenames:
- `baseline_wikitext103_validation_all_500batches_<timestamp>.json`
- `baseline_synthetic_ny_adj_50samples_<timestamp>.json`

---

## Next Steps

1. ‚úÖ **Baseline evaluations** - Completed
2. ‚è≥ **Implement attention wrapper** - Replace BitNetAttention with wrapper
3. ‚è≥ **Implement bit-sampling LSH** - Basic bucket-based attention
4. ‚è≥ **Compare efficiency vs accuracy** - Measure attention comparisons
5. ‚è≥ **Implement LSH Forest** - Dynamic candidate selection
6. ‚è≥ **Comprehensive comparison** - Full vs Bucket vs Forest attention

---

## Notes

- **Model Cache**: The `models/` directory contains HuggingFace cached model files (data)
- **Code**: The `src/model_adapters/` directory contains Python code for model adapters
- **GQA**: BitNet uses Grouped Query Attention, which our wrapper will handle correctly
- **Quantization**: BitNet uses 1.58-bit quantization (`AutoBitLinear`), which we preserve
