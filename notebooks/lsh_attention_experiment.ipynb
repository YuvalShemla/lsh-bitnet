{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Half-Space LSH Attention with BitNet\n",
    "\n",
    "This notebook demonstrates:\n",
    "1. How to use BitNet for text generation\n",
    "2. How FlexAttention enables custom LSH attention patterns\n",
    "3. How to integrate LSH attention into BitNet\n",
    "4. Comparison of results with and without LSH attention\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports\n",
    "\n",
    "**Important Notes:**\n",
    "- **accelerate package**: BitNet models require the `accelerate` package. Install with: `pip install accelerate`\n",
    "- **⚠️ If you get an \"accelerate not found\" error**: **Restart the Jupyter kernel** (Kernel → Restart) and re-run all cells. This is required because `transformers` checks for `accelerate` at import time.\n",
    "- To install all requirements: `pip install -r requirements.txt`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yuvalshemla/Desktop/sparse-attention/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ accelerate version: 1.12.0\n",
      "PyTorch version: 2.9.1\n",
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# CRITICAL: Import accelerate BEFORE transformers\n",
    "# Transformers checks for accelerate at import time, so it must be imported first\n",
    "import accelerate\n",
    "print(f\"✓ accelerate version: {accelerate.__version__}\")\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from torch.nn.attention.flex_attention import flex_attention\n",
    "import numpy as np\n",
    "from typing import Optional\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "\n",
    "# Use CPU for BitNet (works best on CPU)\n",
    "device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Simple BitNet Call\n",
    "\n",
    "Let's start by loading BitNet and making a simple generation call to see how it works.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model will be loaded from: /Users/yuvalshemla/Desktop/models\n"
     ]
    }
   ],
   "source": [
    "# Set up model paths\n",
    "script_dir = os.path.dirname(os.path.abspath('.'))\n",
    "models_cache_dir = os.path.join(script_dir, \"models\")\n",
    "model_id = \"microsoft/bitnet-b1.58-2B-4T\"\n",
    "\n",
    "print(f\"Model will be loaded from: {models_cache_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You don't have a GPU available to load the model, the inference will be slow because of weight unpacking\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model with dtype: torch.bfloat16...\n",
      "✓ Model loaded and moved to device: cpu\n",
      "Model config: bitnet\n",
      "Hidden size: 2560\n",
      "Number of attention heads: 20\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, cache_dir=models_cache_dir)\n",
    "\n",
    "if device.type == \"mps\":\n",
    "    model_dtype = torch.float16\n",
    "else:\n",
    "    model_dtype = torch.bfloat16\n",
    "\n",
    "print(f\"Loading model with dtype: {model_dtype}...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    dtype=model_dtype,\n",
    "    cache_dir=models_cache_dir\n",
    ")\n",
    "\n",
    "# Move model to device\n",
    "model = model.to(device)\n",
    "print(f\"✓ Model loaded and moved to device: {device}\")\n",
    "print(f\"Model config: {model.config.model_type}\")\n",
    "print(f\"Hidden size: {model.config.hidden_size}\")\n",
    "print(f\"Number of attention heads: {model.config.num_attention_heads}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: The future of AI is\n",
      "\n",
      "Generating response...\n",
      "\n",
      "Response: The future of AI is not just about creating machines that can think like humans but also about creating machines that can understand the world in a more holistic way. This means that AI will have to be able to process and analyze large amounts of data, understand natural language, and interact\n"
     ]
    }
   ],
   "source": [
    "# Simple generation example\n",
    "prompt = \"The future of AI is\"\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(\"\\nGenerating response...\")\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        inputs.input_ids,\n",
    "        max_new_tokens=50,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(f\"\\nResponse: {response}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: FlexAttention with Custom LSH Functions\n",
    "\n",
    "Now let's understand how FlexAttention works and implement the simplest half-space LSH example.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSH Oracle: Modular Design\n",
    "\n",
    "Before implementing the half-space LSH, let's create a clean, extensible architecture. The LSH Oracle will:\n",
    "1. **Pre-compute key signatures** once (for all keys)\n",
    "2. **Provide a query interface** to determine which keys to attend to\n",
    "3. **Be easily extensible** for testing different LSH methods\n",
    "\n",
    "This design separates the LSH logic from the attention mechanism, making it easy to experiment with different approaches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Created LSHOracle base class and HalfSpaceLSHOracle implementation\n"
     ]
    }
   ],
   "source": [
    "class LSHOracle(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Base class for LSH attention oracles.\n",
    "    \n",
    "    An LSH oracle pre-computes signatures for all keys and provides\n",
    "    a query interface to determine which keys should be attended to.\n",
    "    \n",
    "    This design allows easy experimentation with different LSH methods.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_heads: int, head_dim: int):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_heads: Number of attention heads\n",
    "            head_dim: Dimension of each head\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = head_dim\n",
    "        self.key_signatures = None  # Will be set by compute_key_signatures\n",
    "    \n",
    "    def compute_key_signatures(self, keys: torch.Tensor, hyperplane_w: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Pre-compute LSH signatures for all keys.\n",
    "        \n",
    "        This is called once per forward pass with all keys.\n",
    "        \n",
    "        Args:\n",
    "            keys: Key tensor [batch, num_heads, seq_len, head_dim]\n",
    "            hyperplane_w: Hyperplane vectors [num_heads, head_dim]\n",
    "            \n",
    "        Returns:\n",
    "            key_signatures: Pre-computed signatures (format depends on implementation)\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Subclasses must implement compute_key_signatures\")\n",
    "    \n",
    "    def should_attend(self, batch: int, head: int, q_idx: int, k_idx: int) -> bool:\n",
    "        \"\"\"\n",
    "        Determine if a query should attend to a key.\n",
    "        \n",
    "        Args:\n",
    "            batch: Batch index\n",
    "            head: Head index\n",
    "            q_idx: Query position index\n",
    "            k_idx: Key position index\n",
    "            \n",
    "        Returns:\n",
    "            True if query should attend to key, False otherwise\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Subclasses must implement should_attend\")\n",
    "    \n",
    "    def create_score_mod(self, query_signatures: torch.Tensor, key_signatures: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Create a score_mod function for FlexAttention.\n",
    "        \n",
    "        This is a convenience method that wraps should_attend for use with FlexAttention.\n",
    "        \n",
    "        Args:\n",
    "            query_signatures: Query signatures [batch, num_heads, seq_len_q]\n",
    "            key_signatures: Key signatures [batch, num_heads, seq_len_k]\n",
    "            \n",
    "        Returns:\n",
    "            score_mod function compatible with FlexAttention\n",
    "        \"\"\"\n",
    "        # Store signatures for lookups\n",
    "        self.query_signatures = query_signatures\n",
    "        self.key_signatures = key_signatures\n",
    "        \n",
    "        def score_mod(score, batch, head, q_idx, k_idx):\n",
    "            # Use should_attend to determine masking\n",
    "            if self.should_attend(batch, head, q_idx, k_idx):\n",
    "                return score\n",
    "            else:\n",
    "                return torch.tensor(float('-inf'), device=score.device, dtype=score.dtype)\n",
    "        \n",
    "        return score_mod\n",
    "\n",
    "\n",
    "class HalfSpaceLSHOracle(LSHOracle):\n",
    "    \"\"\"\n",
    "    Half-space LSH oracle.\n",
    "    \n",
    "    Uses a single random hyperplane per head. Queries attend to keys\n",
    "    in the same half-space (same sign of dot product with hyperplane).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_heads: int, head_dim: int, hyperplane_w: torch.Tensor = None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_heads: Number of attention heads\n",
    "            head_dim: Dimension of each head\n",
    "            hyperplane_w: Hyperplane vectors [num_heads, head_dim]. If None, random.\n",
    "        \"\"\"\n",
    "        super().__init__(num_heads, head_dim)\n",
    "        \n",
    "        if hyperplane_w is None:\n",
    "            # Initialize random hyperplanes (will be moved to device when module is moved)\n",
    "            hyperplane_w = torch.randn(num_heads, head_dim)\n",
    "            hyperplane_w = hyperplane_w / (hyperplane_w.norm(dim=-1, keepdim=True) + 1e-8)\n",
    "        \n",
    "        self.register_buffer('hyperplane_w', hyperplane_w)\n",
    "    \n",
    "    def compute_key_signatures(self, keys: torch.Tensor, hyperplane_w: torch.Tensor = None):\n",
    "        \"\"\"\n",
    "        Compute half-space signatures for all keys.\n",
    "        \n",
    "        Args:\n",
    "            keys: Key tensor [batch, num_heads, seq_len, head_dim]\n",
    "            hyperplane_w: Hyperplane vectors [num_heads, head_dim]. If None, use self.hyperplane_w\n",
    "            \n",
    "        Returns:\n",
    "            key_signs: Key signatures [batch, num_heads, seq_len] with values -1, 0, or 1\n",
    "        \"\"\"\n",
    "        if hyperplane_w is None:\n",
    "            hyperplane_w = self.hyperplane_w\n",
    "        \n",
    "        # Compute dot products: [batch, num_heads, seq_len]\n",
    "        key_dots = torch.einsum('bhsf,hf->bhs', keys, hyperplane_w)\n",
    "        # Get signs: -1, 0, or 1\n",
    "        key_signs = torch.sign(key_dots).detach()\n",
    "        \n",
    "        self.key_signatures = key_signs\n",
    "        return key_signs\n",
    "    \n",
    "    def compute_query_signatures(self, queries: torch.Tensor, hyperplane_w: torch.Tensor = None):\n",
    "        \"\"\"\n",
    "        Compute half-space signatures for all queries.\n",
    "        \n",
    "        Args:\n",
    "            queries: Query tensor [batch, num_heads, seq_len, head_dim]\n",
    "            hyperplane_w: Hyperplane vectors [num_heads, head_dim]. If None, use self.hyperplane_w\n",
    "            \n",
    "        Returns:\n",
    "            query_signs: Query signatures [batch, num_heads, seq_len] with values -1, 0, or 1\n",
    "        \"\"\"\n",
    "        if hyperplane_w is None:\n",
    "            hyperplane_w = self.hyperplane_w\n",
    "        \n",
    "        # Compute dot products: [batch, num_heads, seq_len]\n",
    "        query_dots = torch.einsum('bhlf,hf->bhl', queries, hyperplane_w)\n",
    "        # Get signs: -1, 0, or 1\n",
    "        query_signs = torch.sign(query_dots).detach()\n",
    "        \n",
    "        # Store for should_attend method\n",
    "        self.query_signatures = query_signs\n",
    "        return query_signs\n",
    "    \n",
    "    def should_attend(self, batch: int, head: int, q_idx: int, k_idx: int) -> bool:\n",
    "        \"\"\"\n",
    "        Check if query and key are in the same half-space.\n",
    "        \n",
    "        Args:\n",
    "            batch: Batch index\n",
    "            head: Head index\n",
    "            q_idx: Query position index\n",
    "            k_idx: Key position index\n",
    "            \n",
    "        Returns:\n",
    "            True if signs match (same half-space), False otherwise\n",
    "        \"\"\"\n",
    "        if self.query_signatures is None or self.key_signatures is None:\n",
    "            raise ValueError(\"Must call compute_key_signatures and set query_signatures first\")\n",
    "        \n",
    "        q_sign = self.query_signatures[batch, head, q_idx]\n",
    "        k_sign = self.key_signatures[batch, head, k_idx]\n",
    "        \n",
    "        # Signs match if their product is positive\n",
    "        return (q_sign * k_sign > 0.5).item()\n",
    "    \n",
    "    def create_score_mod(self, query_signatures: torch.Tensor, key_signatures: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Create score_mod function for FlexAttention (optimized version).\n",
    "        \n",
    "        This version directly uses the pre-computed signatures without calling should_attend\n",
    "        for better performance.\n",
    "        \"\"\"\n",
    "        def score_mod(score, batch, head, q_idx, k_idx):\n",
    "            q_sign = query_signatures[batch, head, q_idx]\n",
    "            k_sign = key_signatures[batch, head, k_idx]\n",
    "            sign_match = (q_sign * k_sign > 0.5).float()\n",
    "            \n",
    "            return torch.where(\n",
    "                sign_match > 0.5,\n",
    "                score,\n",
    "                torch.tensor(float('-inf'), device=score.device, dtype=score.dtype)\n",
    "            )\n",
    "        \n",
    "        return score_mod\n",
    "\n",
    "print(\"✓ Created LSHOracle base class and HalfSpaceLSHOracle implementation\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the LSH Oracle\n",
    "\n",
    "Now let's use the oracle to implement half-space LSH attention. Notice how clean and modular this is!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oracle hyperplane shape: torch.Size([2, 16])\n",
      "Hyperplane (head 0): [-0.23390657  0.17189293 -0.13955499  0.43347573  0.3144951 ]\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Create the LSH oracle\n",
    "# First, define the parameters for our simple example\n",
    "batch_size = 1\n",
    "num_heads = 2\n",
    "seq_len = 8\n",
    "head_dim = 16\n",
    "\n",
    "oracle = HalfSpaceLSHOracle(num_heads=num_heads, head_dim=head_dim)\n",
    "\n",
    "# The oracle has hyperplane vectors initialized\n",
    "print(f\"Oracle hyperplane shape: {oracle.hyperplane_w.shape}\")\n",
    "print(f\"Hyperplane (head 0): {oracle.hyperplane_w[0, :5].numpy()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key signatures shape: torch.Size([1, 2, 8])\n",
      "Key signatures (head 0): [ 1.  1.  1. -1.  1. -1. -1.  1.]\n",
      "Unique values: [-1.  1.]\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Pre-compute key signatures (done once for all keys)\n",
    "# First, create Q, K, V tensors for the example\n",
    "Q = torch.randn(batch_size, num_heads, seq_len, head_dim)\n",
    "K = torch.randn(batch_size, num_heads, seq_len, head_dim)\n",
    "V = torch.randn(batch_size, num_heads, seq_len, head_dim)\n",
    "\n",
    "# Now compute key signatures\n",
    "key_signatures = oracle.compute_key_signatures(K, oracle.hyperplane_w)\n",
    "print(f\"Key signatures shape: {key_signatures.shape}\")\n",
    "print(f\"Key signatures (head 0): {key_signatures[0, 0].numpy()}\")\n",
    "print(f\"Unique values: {torch.unique(key_signatures).numpy()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query signatures shape: torch.Size([1, 2, 8])\n",
      "Query signatures (head 0): [ 1. -1. -1. -1.  1. -1.  1. -1.]\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Compute query signatures (done once for all queries)\n",
    "query_signatures = oracle.compute_query_signatures(Q, oracle.hyperplane_w)\n",
    "print(f\"Query signatures shape: {query_signatures.shape}\")\n",
    "print(f\"Query signatures (head 0): {query_signatures[0, 0].numpy()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Created LSH score_mod function using oracle\n",
      "\n",
      "Testing should_attend:\n",
      "  Query 0, Key 0: True\n",
      "  Query 0, Key 1: True\n",
      "  Query 1, Key 0: False\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Create score_mod function using the oracle\n",
    "\n",
    "lsh_score_mod = oracle.create_score_mod(query_signatures, key_signatures)\n",
    "print(\"✓ Created LSH score_mod function using oracle\")\n",
    "\n",
    "# Test the oracle's should_attend method\n",
    "# The signatures are now stored in the oracle from compute_key_signatures and compute_query_signatures\n",
    "print(f\"\\nTesting should_attend:\")\n",
    "print(f\"  Query 0, Key 0: {oracle.should_attend(0, 0, 0, 0)}\")\n",
    "print(f\"  Query 0, Key 1: {oracle.should_attend(0, 0, 0, 1)}\")\n",
    "print(f\"  Query 1, Key 0: {oracle.should_attend(0, 0, 1, 0)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Half-Space LSH: The Simplest Example\n",
    "\n",
    "**Concept**: \n",
    "1. Sample a random hyperplane vector `w`\n",
    "2. For each query `q`, compute `sign(dot(w, q))`\n",
    "3. For each key `k`, compute `sign(dot(w, k))`\n",
    "4. Only allow attention if signs match (same half-space)\n",
    "\n",
    "This creates a sparse attention pattern where queries only attend to \"similar\" keys (in the same half-space).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperplane shape: torch.Size([2, 16])\n",
      "Hyperplane (head 0): [-0.44362113 -0.0333774   0.36112836  0.33516774  0.0362303 ]\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Sample a random hyperplane vector\n",
    "hyperplane_w = torch.randn(num_heads, head_dim)\n",
    "# Normalize it\n",
    "hyperplane_w = hyperplane_w / (hyperplane_w.norm(dim=-1, keepdim=True) + 1e-8)\n",
    "print(f\"Hyperplane shape: {hyperplane_w.shape}\")\n",
    "print(f\"Hyperplane (head 0): {hyperplane_w[0, :5].numpy()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query signs shape: torch.Size([1, 2, 8])\n",
      "Query signs (head 0): [ 1. -1.  1.  1.  1. -1.  1.  1.]\n",
      "Key signs (head 0): [ 1. -1.  1. -1. -1.  1.  1.  1.]\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Compute signs for queries and keys\n",
    "# For each query: sign(dot(w, q))\n",
    "query_dots = torch.einsum('bhlf,hf->bhl', Q, hyperplane_w)\n",
    "query_signs = torch.sign(query_dots).detach()  # Detach to avoid gradient issues\n",
    "\n",
    "# For each key: sign(dot(w, k))\n",
    "key_dots = torch.einsum('bhsf,hf->bhs', K, hyperplane_w)\n",
    "key_signs = torch.sign(key_dots).detach()\n",
    "\n",
    "print(f\"Query signs shape: {query_signs.shape}\")\n",
    "print(f\"Query signs (head 0): {query_signs[0, 0].numpy()}\")\n",
    "print(f\"Key signs (head 0): {key_signs[0, 0].numpy()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Created LSH score_mod function\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Create score_mod function that masks mismatched signs\n",
    "def create_lsh_score_mod(query_signs, key_signs):\n",
    "    \"\"\"\n",
    "    Creates a score_mod function for half-space LSH attention.\n",
    "    \n",
    "    If query and key are in the same half-space (signs match),\n",
    "    return the original score. Otherwise, return -inf (masked).\n",
    "    \"\"\"\n",
    "    def score_mod(score, batch, head, q_idx, k_idx):\n",
    "        # Look up the signs for this (query, key) pair\n",
    "        q_sign = query_signs[batch, head, q_idx]\n",
    "        k_sign = key_signs[batch, head, k_idx]\n",
    "        \n",
    "        # Check if signs match (same half-space)\n",
    "        # sign_match = 1 if q_sign * k_sign > 0, else 0\n",
    "        sign_match = (q_sign * k_sign > 0.5).float()\n",
    "        \n",
    "        # If signs match, return original score; otherwise mask with -inf\n",
    "        return torch.where(\n",
    "            sign_match > 0.5,\n",
    "            score,\n",
    "            torch.tensor(float('-inf'), device=score.device, dtype=score.dtype)\n",
    "        )\n",
    "    \n",
    "    return score_mod\n",
    "\n",
    "# Create the score_mod function\n",
    "lsh_score_mod = create_lsh_score_mod(query_signs, key_signs)\n",
    "print(\"✓ Created LSH score_mod function\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard attention output shape: torch.Size([1, 2, 8, 16])\n",
      "Standard attention norm: 6.8566\n",
      "\n",
      "LSH attention output shape: torch.Size([1, 2, 8, 16])\n",
      "LSH attention norm: 8.4556\n",
      "\n",
      "Mean absolute difference from standard attention: 0.1947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yuvalshemla/Desktop/sparse-attention/.venv/lib/python3.13/site-packages/torch/nn/attention/flex_attention.py:1687: UserWarning: flex_attention called without torch.compile() - this will use an unfused implementation that materializes the full scores matrix instead of generating a fused kernel.\n",
      "\n",
      "SOLUTION: Use torch.compile(flex_attention)(...)\n",
      "\n",
      "If you want to debug your score_mod/mask_mod, you can set:\n",
      "torch.nn.attention.flex_attention._FLEX_ATTENTION_DISABLE_COMPILE_DEBUG = True\n",
      "\n",
      "This will allow you to use print statements or breakpoints. Note: This doesn't work with the backwards pass and may produce incorrect results.\n",
      "  _warn_once(\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Apply FlexAttention with LSH masking (using oracle)\n",
    "# Note: You may see a warning about torch.compile() - this is normal for debugging.\n",
    "# For production, you can use: torch.compile(flex_attention)(...) for better performance.\n",
    "\n",
    "# First, compute standard attention for comparison\n",
    "output_standard = flex_attention(Q, K, V, scale=1.0 / (head_dim ** 0.5))\n",
    "print(f\"Standard attention output shape: {output_standard.shape}\")\n",
    "print(f\"Standard attention norm: {output_standard.norm().item():.4f}\")\n",
    "\n",
    "# Now compute LSH attention\n",
    "output_lsh = flex_attention(\n",
    "    Q, K, V,\n",
    "    score_mod=lsh_score_mod,\n",
    "    scale=1.0 / (head_dim ** 0.5)\n",
    ")\n",
    "\n",
    "print(f\"\\nLSH attention output shape: {output_lsh.shape}\")\n",
    "print(f\"LSH attention norm: {output_lsh.norm().item():.4f}\")\n",
    "\n",
    "# Compare with standard attention\n",
    "diff = (output_standard - output_lsh).abs().mean()\n",
    "print(f\"\\nMean absolute difference from standard attention: {diff.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Easy Extension for Testing Other LSH Methods\n",
    "\n",
    "The oracle design makes it trivial to test different LSH approaches. Here's an example of how you could add a multi-hyperplane LSH:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Example: MultiHyperplaneLSHOracle class created\n",
      "  This shows how easy it is to extend the oracle for different LSH methods!\n",
      "  To use it, just replace HalfSpaceLSHOracle with MultiHyperplaneLSHOracle\n"
     ]
    }
   ],
   "source": [
    "class MultiHyperplaneLSHOracle(LSHOracle):\n",
    "    \"\"\"\n",
    "    Example: Multi-hyperplane LSH oracle.\n",
    "    \n",
    "    Uses multiple hyperplanes per head. A query attends to a key\n",
    "    if they match on at least k hyperplanes.\n",
    "    \n",
    "    This demonstrates how easy it is to extend the oracle design!\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_heads: int, head_dim: int, num_hyperplanes: int = 2, min_matches: int = 1):\n",
    "        super().__init__(num_heads, head_dim)\n",
    "        self.num_hyperplanes = num_hyperplanes\n",
    "        self.min_matches = min_matches\n",
    "        \n",
    "        # Initialize multiple hyperplanes per head\n",
    "        # Will be moved to device when module is moved via .to(device)\n",
    "        hyperplanes = torch.randn(num_heads, num_hyperplanes, head_dim)\n",
    "        hyperplanes = hyperplanes / (hyperplanes.norm(dim=-1, keepdim=True) + 1e-8)\n",
    "        self.register_buffer('hyperplanes', hyperplanes)\n",
    "    \n",
    "    def compute_key_signatures(self, keys: torch.Tensor, hyperplanes: torch.Tensor = None):\n",
    "        if hyperplanes is None:\n",
    "            hyperplanes = self.hyperplanes\n",
    "        \n",
    "        # Compute signs for each hyperplane: [batch, num_heads, num_hyperplanes, seq_len]\n",
    "        key_dots = torch.einsum('bhsf,hnf->bhns', keys, hyperplanes)\n",
    "        key_signs = torch.sign(key_dots).detach()\n",
    "        \n",
    "        self.key_signatures = key_signs\n",
    "        return key_signs\n",
    "    \n",
    "    def compute_query_signatures(self, queries: torch.Tensor, hyperplanes: torch.Tensor = None):\n",
    "        if hyperplanes is None:\n",
    "            hyperplanes = self.hyperplanes\n",
    "        \n",
    "        # Compute signs for each hyperplane: [batch, num_heads, num_hyperplanes, seq_len]\n",
    "        query_dots = torch.einsum('bhlf,hnf->bhnl', queries, hyperplanes)\n",
    "        query_signs = torch.sign(query_dots).detach()\n",
    "        \n",
    "        return query_signs\n",
    "    \n",
    "    def should_attend(self, batch: int, head: int, q_idx: int, k_idx: int) -> bool:\n",
    "        if self.query_signatures is None or self.key_signatures is None:\n",
    "            raise ValueError(\"Must compute signatures first\")\n",
    "        \n",
    "        # Check how many hyperplanes match\n",
    "        q_signs = self.query_signatures[batch, head, :, q_idx]  # [num_hyperplanes]\n",
    "        k_signs = self.key_signatures[batch, head, :, k_idx]    # [num_hyperplanes]\n",
    "        \n",
    "        matches = (q_signs * k_signs > 0.5).sum().item()\n",
    "        return matches >= self.min_matches\n",
    "    \n",
    "    def create_score_mod(self, query_signatures: torch.Tensor, key_signatures: torch.Tensor):\n",
    "        def score_mod(score, batch, head, q_idx, k_idx):\n",
    "            q_signs = query_signatures[batch, head, :, q_idx]\n",
    "            k_signs = key_signatures[batch, head, :, k_idx]\n",
    "            matches = (q_signs * k_signs > 0.5).sum()\n",
    "            \n",
    "            if matches >= self.min_matches:\n",
    "                return score\n",
    "            else:\n",
    "                return torch.tensor(float('-inf'), device=score.device, dtype=score.dtype)\n",
    "        \n",
    "        return score_mod\n",
    "\n",
    "print(\"✓ Example: MultiHyperplaneLSHOracle class created\")\n",
    "print(\"  This shows how easy it is to extend the oracle for different LSH methods!\")\n",
    "print(\"  To use it, just replace HalfSpaceLSHOracle with MultiHyperplaneLSHOracle\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Masking ratio: 53.1%\n",
      "Expected: ~50% (random hyperplane splits space in half)\n"
     ]
    }
   ],
   "source": [
    "# Visualize the masking pattern\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Compute which pairs are masked (for visualization)\n",
    "sign_match_matrix = (query_signatures.unsqueeze(-1) * key_signatures.unsqueeze(-2) > 0.5).float()\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(sign_match_matrix[0, 0].cpu().numpy(), cmap='RdYlGn', vmin=0, vmax=1)\n",
    "plt.title(f\"LSH Mask Pattern (Head 0)\\nRed=Masked, Green=Allowed\")\n",
    "plt.xlabel(\"Key Position\")\n",
    "plt.ylabel(\"Query Position\")\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "masking_ratio = (1 - sign_match_matrix.mean()).item() * 100\n",
    "plt.bar(['Masked', 'Allowed'], \n",
    "        [masking_ratio, 100 - masking_ratio], \n",
    "        color=['red', 'green'])\n",
    "plt.title(f\"Masking Statistics\\n{masking_ratio:.1f}% masked\")\n",
    "plt.ylabel(\"Percentage\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nMasking ratio: {masking_ratio:.1f}%\")\n",
    "print(f\"Expected: ~50% (random hyperplane splits space in half)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding FlexAttention\n",
    "\n",
    "FlexAttention allows us to modify attention scores element-wise using a `score_mod` function:\n",
    "\n",
    "```python\n",
    "def score_mod(score, batch, head, q_idx, k_idx) -> Tensor:\n",
    "    # Modify the attention score here\n",
    "    return modified_score\n",
    "```\n",
    "\n",
    "This function receives:\n",
    "- `score`: The computed attention score (scalar)\n",
    "- `batch`, `head`, `q_idx`, `k_idx`: Indices for the current position\n",
    "\n",
    "We can use this to implement custom attention patterns like LSH!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q shape: torch.Size([1, 2, 8, 16])\n",
      "K shape: torch.Size([1, 2, 8, 16])\n",
      "V shape: torch.Size([1, 2, 8, 16])\n"
     ]
    }
   ],
   "source": [
    "# Simple example: Standard attention with FlexAttention\n",
    "# Note: batch_size, num_heads, seq_len, head_dim are already defined earlier\n",
    "\n",
    "# Create random Q, K, V\n",
    "Q = torch.randn(batch_size, num_heads, seq_len, head_dim)\n",
    "K = torch.randn(batch_size, num_heads, seq_len, head_dim)\n",
    "V = torch.randn(batch_size, num_heads, seq_len, head_dim)\n",
    "\n",
    "print(f\"Q shape: {Q.shape}\")\n",
    "print(f\"K shape: {K.shape}\")\n",
    "print(f\"V shape: {V.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard attention output shape: torch.Size([1, 2, 8, 16])\n",
      "Output norm: 6.3493\n"
     ]
    }
   ],
   "source": [
    "# Standard attention (no score_mod)\n",
    "output_standard = flex_attention(Q, K, V, scale=1.0 / (head_dim ** 0.5))\n",
    "print(f\"Standard attention output shape: {output_standard.shape}\")\n",
    "print(f\"Output norm: {output_standard.norm().item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: What We Need to Change in BitNet\n",
    "\n",
    "To integrate LSH attention into BitNet, we need to:\n",
    "\n",
    "1. **Inspect the model structure** to find where attention is implemented\n",
    "2. **Identify the attention module** (likely `self_attn` or `attn` in each layer)\n",
    "3. **Create an LSH attention wrapper** that matches the interface\n",
    "4. **Replace the attention computation** in the forward pass\n",
    "\n",
    "Let's inspect BitNet's structure:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model type: BitNetForCausalLM\n",
      "\n",
      "Model attributes:\n",
      "  T_destination: TypeVar\n",
      "  base_model_prefix: str\n",
      "  call_super_init: bool\n",
      "  can_record_outputs: dict\n",
      "  config: BitNetConfig\n",
      "  device: device\n",
      "  dtype: dtype\n",
      "  dummy_inputs: dict\n",
      "  dump_patches: bool\n",
      "  framework: str\n",
      "  generation_config: GenerationConfig\n",
      "  hf_quantizer: BitNetHfQuantizer\n",
      "  is_gradient_checkpointing: bool\n",
      "  is_parallelizable: bool\n",
      "  is_quantized: bool\n",
      "  loss_type: str\n",
      "  main_input_name: str\n",
      "  model_tags: NoneType\n",
      "  name_or_path: str\n",
      "  pp_plan: dict\n",
      "  quantization_method: QuantizationMethod\n",
      "  supports_gradient_checkpointing: bool\n",
      "  supports_pp_plan: bool\n",
      "  supports_tp_plan: bool\n",
      "  tp_plan: dict\n",
      "  tp_size: NoneType\n",
      "  training: bool\n",
      "  use_kernels: bool\n",
      "  vocab_size: int\n",
      "  warnings_issued: dict\n"
     ]
    }
   ],
   "source": [
    "# Inspect BitNet's structure\n",
    "print(\"Model type:\", type(model).__name__)\n",
    "print(\"\\nModel attributes:\")\n",
    "for attr in dir(model):\n",
    "    if not attr.startswith('_'):\n",
    "        obj = getattr(model, attr)\n",
    "        if not callable(obj):\n",
    "            print(f\"  {attr}: {type(obj).__name__}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found layers at: model.model.layers\n",
      "Number of layers: 30\n"
     ]
    }
   ],
   "source": [
    "# Find the transformer layers\n",
    "if hasattr(model, 'model') and hasattr(model.model, 'layers'):\n",
    "    layers = model.model.layers\n",
    "    print(f\"Found layers at: model.model.layers\")\n",
    "    print(f\"Number of layers: {len(layers)}\")\n",
    "elif hasattr(model, 'transformer') and hasattr(model.transformer, 'h'):\n",
    "    layers = model.transformer.h\n",
    "    print(f\"Found layers at: model.transformer.h\")\n",
    "    print(f\"Number of layers: {len(layers)}\")\n",
    "else:\n",
    "    print(\"Searching for layers...\")\n",
    "    for name, module in model.named_modules():\n",
    "        if 'layer' in name.lower() and isinstance(module, torch.nn.ModuleList):\n",
    "            layers = module\n",
    "            print(f\"Found layers at: {name}\")\n",
    "            print(f\"Number of layers: {len(layers)}\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First layer type: BitNetDecoderLayer\n",
      "\n",
      "First layer attributes (all):\n",
      "  T_destination: TypeVar = ~T_destination\n",
      "  call_super_init: bool = False\n",
      "  dump_patches: bool = False\n",
      "  gradient_checkpointing: bool = False\n",
      "  hidden_size: int = 2560\n",
      "  input_layernorm: BitNetRMSNorm\n",
      "  mlp: BitNetMLP\n",
      "  post_attention_layernorm: BitNetRMSNorm\n",
      "  self_attn: BitNetAttention\n",
      "  training: bool = False\n",
      "\n",
      "First layer modules (using named_modules):\n",
      "  self_attn: BitNetAttention\n",
      "  self_attn.q_proj: AutoBitLinear\n",
      "  self_attn.k_proj: AutoBitLinear\n",
      "  self_attn.v_proj: AutoBitLinear\n",
      "  self_attn.o_proj: AutoBitLinear\n",
      "  self_attn.attn_sub_norm: BitNetRMSNorm\n",
      "  mlp: BitNetMLP\n",
      "  mlp.gate_proj: AutoBitLinear\n",
      "  mlp.up_proj: AutoBitLinear\n",
      "  mlp.down_proj: AutoBitLinear\n",
      "  mlp.act_fn: ReLUSquaredActivation\n",
      "  mlp.ffn_sub_norm: BitNetRMSNorm\n",
      "  input_layernorm: BitNetRMSNorm\n",
      "  post_attention_layernorm: BitNetRMSNorm\n"
     ]
    }
   ],
   "source": [
    "# Inspect the first layer to understand its structure\n",
    "first_layer = layers[0]\n",
    "print(\"First layer type:\", type(first_layer).__name__)\n",
    "print(\"\\nFirst layer attributes (all):\")\n",
    "for attr in dir(first_layer):\n",
    "    if not attr.startswith('_'):\n",
    "        try:\n",
    "            obj = getattr(first_layer, attr)\n",
    "            if isinstance(obj, torch.nn.Module):\n",
    "                print(f\"  {attr}: {type(obj).__name__}\")\n",
    "            elif not callable(obj):\n",
    "                print(f\"  {attr}: {type(obj).__name__} = {obj}\")\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "# Also try using named_modules for a cleaner view\n",
    "print(\"\\nFirst layer modules (using named_modules):\")\n",
    "for name, module in first_layer.named_modules():\n",
    "    if name:  # Skip the root module itself\n",
    "        print(f\"  {name}: {type(module).__name__}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found attention at: self_attn\n",
      "\n",
      "Attention module type: BitNetAttention\n",
      "Attention module: BitNetAttention(\n",
      "  (q_proj): AutoBitLinear(in_features=2560, out_features=2560, bias=False)\n",
      "  (k_proj): AutoBitLinear(in_features=2560, out_features=640, bias=False)\n",
      "  (v_proj): AutoBitLinear(in_features=2560, out_features=640, bias=False)\n",
      "  (o_proj): AutoBitLinear(in_features=2560, out_features=2560, bias=False)\n",
      "  (attn_sub_norm): BitNetRMSNorm((2560,), eps=1e-05)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Find the attention module\n",
    "if hasattr(first_layer, 'self_attn'):\n",
    "    attn_module = first_layer.self_attn\n",
    "    attn_name = 'self_attn'\n",
    "    print(f\"Found attention at: self_attn\")\n",
    "elif hasattr(first_layer, 'attn'):\n",
    "    attn_module = first_layer.attn\n",
    "    attn_name = 'attn'\n",
    "    print(f\"Found attention at: attn\")\n",
    "else:\n",
    "    print(\"Could not find attention module. Available attributes:\")\n",
    "    for attr in dir(first_layer):\n",
    "        if 'attn' in attr.lower():\n",
    "            print(f\"  {attr}\")\n",
    "\n",
    "print(f\"\\nAttention module type: {type(attn_module).__name__}\")\n",
    "print(f\"Attention module: {attn_module}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention forward signature:\n",
      "(hidden_states: torch.Tensor, position_embeddings: tuple[torch.Tensor, torch.Tensor], attention_mask: Optional[torch.Tensor], past_key_values: Optional[transformers.cache_utils.Cache] = None, cache_position: Optional[torch.LongTensor] = None, **kwargs: Unpack[transformers.modeling_flash_attention_utils.FlashAttentionKwargs]) -> tuple[torch.Tensor, typing.Optional[torch.Tensor]]\n",
      "\n",
      "Checking forward method source...\n",
      "First few lines:\n",
      "    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n",
      "    def forward(\n",
      "        self,\n",
      "        hidden_states: torch.Tensor,\n",
      "        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n",
      "        attention_mask: Optional[torch.Tensor],\n",
      "        past_key_values: Optional[Cache] = None,\n",
      "        cache_position: Optional[torch.LongTensor] = None,\n",
      "        **kwargs: Unpack[FlashAttentionKwargs],\n",
      "    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n"
     ]
    }
   ],
   "source": [
    "# Check the attention module's forward signature\n",
    "import inspect\n",
    "if hasattr(attn_module, 'forward'):\n",
    "    sig = inspect.signature(attn_module.forward)\n",
    "    print(\"Attention forward signature:\")\n",
    "    print(sig)\n",
    "    \n",
    "    # Try to see what it expects\n",
    "    print(\"\\nChecking forward method source...\")\n",
    "    try:\n",
    "        source = inspect.getsource(attn_module.forward)\n",
    "        print(\"First few lines:\")\n",
    "        print('\\n'.join(source.split('\\n')[:10]))\n",
    "    except:\n",
    "        print(\"Could not get source\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating an LSH Attention Wrapper\n",
    "\n",
    "We need to create a wrapper that:\n",
    "1. Has the same interface as BitNet's attention module\n",
    "2. Uses FlexAttention with LSH masking internally\n",
    "3. Handles Q/K/V projections correctly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Created LSHAttentionWrapper class using LSH Oracle\n",
      "  To use a different LSH method, just pass a different oracle!\n"
     ]
    }
   ],
   "source": [
    "class LSHAttentionWrapper(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Wrapper that replaces standard attention with LSH attention using an LSH Oracle.\n",
    "    \n",
    "    This wrapper uses the modular oracle design, making it easy to swap\n",
    "    different LSH methods by changing the oracle.\n",
    "    \"\"\"\n",
    "    def __init__(self, original_attn, embed_dim, num_heads, head_dim=None, oracle=None):\n",
    "        super().__init__()\n",
    "        self.original_attn = original_attn\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = head_dim if head_dim is not None else embed_dim // num_heads\n",
    "        \n",
    "        # Use provided oracle or create default half-space oracle\n",
    "        if oracle is None:\n",
    "            self.oracle = HalfSpaceLSHOracle(num_heads, self.head_dim)\n",
    "        else:\n",
    "            self.oracle = oracle\n",
    "        \n",
    "        # Note: Oracle will be moved to device when wrapper is moved via .to(device)\n",
    "        \n",
    "        # Copy Q/K/V projections from original attention if they exist\n",
    "        if hasattr(original_attn, 'q_proj'):\n",
    "            self.q_proj = original_attn.q_proj\n",
    "            self.k_proj = original_attn.k_proj\n",
    "            self.v_proj = original_attn.v_proj\n",
    "            self.o_proj = original_attn.o_proj if hasattr(original_attn, 'o_proj') else None\n",
    "        else:\n",
    "            # If structure is different, we'll need to adapt\n",
    "            self.q_proj = None\n",
    "    \n",
    "    def forward(self, hidden_states, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Forward pass with LSH attention using the oracle.\n",
    "        \n",
    "        The oracle pre-computes key signatures once, then determines\n",
    "        which keys each query should attend to.\n",
    "        \"\"\"\n",
    "        # Get Q, K, V from the original attention module's projections\n",
    "        if self.q_proj is None:\n",
    "            # Fallback to original attention if we can't intercept\n",
    "            return self.original_attn(hidden_states, *args, **kwargs)\n",
    "        \n",
    "        batch_size, seq_len, _ = hidden_states.shape\n",
    "        \n",
    "        # Project to Q, K, V\n",
    "        Q = self.q_proj(hidden_states)\n",
    "        K = self.k_proj(hidden_states)\n",
    "        V = self.v_proj(hidden_states)\n",
    "        \n",
    "        # BitNet uses quantized/1.58-bit projections which may output different shapes\n",
    "        # Check if we can reshape to expected format\n",
    "        expected_total = batch_size * seq_len * self.num_heads * self.head_dim\n",
    "        actual_total = Q.numel()\n",
    "        \n",
    "        if actual_total != expected_total:\n",
    "            # BitNet's projection format doesn't match - fall back to original\n",
    "            # This happens because BitNet uses specialized quantized layers\n",
    "            return self.original_attn(hidden_states, *args, **kwargs)\n",
    "        \n",
    "        # Reshape to [B, H, L, head_dim] format for FlexAttention\n",
    "        try:\n",
    "            Q = Q.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "            K = K.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "            V = V.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        except RuntimeError as e:\n",
    "            # If reshaping fails, fall back to original attention\n",
    "            return self.original_attn(hidden_states, *args, **kwargs)\n",
    "        \n",
    "        # Use oracle to compute signatures (pre-compute once)\n",
    "        # Keys are computed first (they're available for all queries)\n",
    "        key_signatures = self.oracle.compute_key_signatures(K)\n",
    "        \n",
    "        # Query signatures are computed per query position\n",
    "        query_signatures = self.oracle.compute_query_signatures(Q)\n",
    "        \n",
    "        # Create score_mod function using the oracle\n",
    "        score_mod = self.oracle.create_score_mod(query_signatures, key_signatures)\n",
    "        \n",
    "        # Apply FlexAttention with LSH\n",
    "        output = flex_attention(\n",
    "            Q, K, V,\n",
    "            score_mod=score_mod,\n",
    "            scale=1.0 / (self.head_dim ** 0.5)\n",
    "        )\n",
    "        \n",
    "        # Reshape back\n",
    "        output = output.transpose(1, 2).contiguous()\n",
    "        output = output.view(batch_size, seq_len, self.embed_dim)\n",
    "        \n",
    "        # Apply output projection if it exists\n",
    "        if self.o_proj is not None:\n",
    "            output = self.o_proj(output)\n",
    "        \n",
    "        # Return in the format expected by the layer\n",
    "        # Usually (output, attention_weights) or just output\n",
    "        return output, None\n",
    "\n",
    "print(\"✓ Created LSHAttentionWrapper class using LSH Oracle\")\n",
    "print(\"  To use a different LSH method, just pass a different oracle!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Actually Implementing the Change\n",
    "\n",
    "Now let's replace BitNet's attention with our LSH attention wrapper. We'll do this for just the first layer as a test.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating LSH attention wrapper...\n",
      "Model config:\n",
      "  Embed dim: 2560\n",
      "  Num heads: 20\n",
      "  Head dim: 128\n"
     ]
    }
   ],
   "source": [
    "# Create a copy of the model for comparison (optional, but good for testing)\n",
    "# For now, we'll modify the model in place\n",
    "\n",
    "print(\"Creating LSH attention wrapper...\")\n",
    "config = model.config\n",
    "embed_dim = config.hidden_size\n",
    "num_heads = config.num_attention_heads\n",
    "head_dim = embed_dim // num_heads\n",
    "\n",
    "print(f\"Model config:\")\n",
    "print(f\"  Embed dim: {embed_dim}\")\n",
    "print(f\"  Num heads: {num_heads}\")\n",
    "print(f\"  Head dim: {head_dim}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replacing attention in layer 0...\n",
      "Original attention type: BitNetAttention\n",
      "✓ Replaced with LSH attention wrapper\n",
      "New attention type: LSHAttentionWrapper\n",
      "Wrapper device: cpu\n",
      "Oracle device: cpu\n",
      "Oracle hyperplane shape: torch.Size([20, 128])\n"
     ]
    }
   ],
   "source": [
    "# Replace attention in the first layer only (for testing)\n",
    "layer_idx = 0\n",
    "target_layer = layers[layer_idx]\n",
    "\n",
    "print(f\"Replacing attention in layer {layer_idx}...\")\n",
    "print(f\"Original attention type: {type(getattr(target_layer, attn_name)).__name__}\")\n",
    "\n",
    "# Store original for comparison\n",
    "original_attn = getattr(target_layer, attn_name)\n",
    "\n",
    "# Create LSH wrapper and move to device\n",
    "# The wrapper will move the oracle to device automatically\n",
    "lsh_wrapper = LSHAttentionWrapper(\n",
    "    original_attn,\n",
    "    embed_dim=embed_dim,\n",
    "    num_heads=num_heads,\n",
    "    head_dim=head_dim\n",
    ").to(device)\n",
    "\n",
    "# Replace the attention module\n",
    "setattr(target_layer, attn_name, lsh_wrapper)\n",
    "\n",
    "print(f\"✓ Replaced with LSH attention wrapper\")\n",
    "print(f\"New attention type: {type(getattr(target_layer, attn_name)).__name__}\")\n",
    "\n",
    "# Check wrapper device (from parameters)\n",
    "try:\n",
    "    wrapper_device = next(lsh_wrapper.parameters()).device\n",
    "    print(f\"Wrapper device: {wrapper_device}\")\n",
    "except StopIteration:\n",
    "    print(\"Wrapper device: No parameters (using buffers)\")\n",
    "\n",
    "# Check oracle device (hyperplane_w is a buffer, not a parameter)\n",
    "if hasattr(lsh_wrapper.oracle, 'hyperplane_w'):\n",
    "    oracle_device = lsh_wrapper.oracle.hyperplane_w.device\n",
    "    print(f\"Oracle device: {oracle_device}\")\n",
    "    print(f\"Oracle hyperplane shape: {lsh_wrapper.oracle.hyperplane_w.shape}\")\n",
    "else:\n",
    "    print(\"Oracle device: N/A (no hyperplane_w found)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Compare Results With and Without LSH\n",
    "\n",
    "Now let's test the model with the same prompt and compare results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading original model for comparison...\n",
      "✓ Original model loaded and moved to device: cpu\n"
     ]
    }
   ],
   "source": [
    "# First, let's restore the original model for fair comparison\n",
    "# We need to reload it to get a clean state\n",
    "print(\"Loading original model for comparison...\")\n",
    "model_original = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    dtype=model_dtype,  # Use same dtype as main model\n",
    "    cache_dir=models_cache_dir\n",
    ").to(device)\n",
    "\n",
    "print(f\"✓ Original model loaded and moved to device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "RESULT 1: ORIGINAL MODEL (Standard Attention)\n",
      "============================================================\n",
      "\n",
      "Response:\n",
      "Answer: At the end of the story, Bob owns a red bicycle, and he acquired it through a series of transactions involving Alice, who initially lent it to him, and David, who bought it back from her. He also acquired it from Charlie\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_prompt = \"\"\"In a small village, there were three friends: Alice, Bob, and Charlie. Alice owned a red bicycle, Bob had a blue bicycle, and Charlie's bicycle was green. One day, Alice lent her red bicycle to Bob, who then gave his blue bicycle to Charlie. Later that week, Charlie returned the blue bicycle to Bob, and Bob gave the red bicycle back to Alice. \n",
    "\n",
    "The following week, Alice decided to sell her red bicycle to a neighbor named David. Bob, who had always wanted a red bicycle, was disappointed. Charlie, feeling sorry for Bob, offered to sell his green bicycle to Bob for a fair price. Bob agreed and bought the green bicycle from Charlie.\n",
    "\n",
    "A month later, David decided he didn't need the red bicycle anymore and sold it back to Alice. Alice, remembering how much Bob had wanted it, immediately offered to give the red bicycle to Bob as a gift. Bob was thrilled and accepted the gift.\n",
    "\n",
    "Question: At the end of this story, what color bicycle does Bob own, and how did he acquire it?\"\"\"\n",
    "\n",
    "# Test with ORIGINAL model (no LSH)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RESULT 1: ORIGINAL MODEL (Standard Attention)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "inputs = tokenizer(test_prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs_original = model_original.generate(\n",
    "        inputs.input_ids,\n",
    "        max_new_tokens=50,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "# Decode only the newly generated tokens (skip the input prompt)\n",
    "input_length = inputs.input_ids.shape[1]\n",
    "response_original = tokenizer.decode(outputs_original[0][input_length:], skip_special_tokens=True)\n",
    "print(f\"\\nResponse:\")\n",
    "print(response_original)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "RESULT 2: LSH MODEL (First Layer with LSH Attention)\n",
      "============================================================\n",
      "\n",
      "Response:\n",
      "Answer: \n",
      "At the end of the story, Bob owns a red bicycle. He acquired it by first borrowing Alice's red bicycle, then receiving a blue bicycle from Bob, giving the blue bicycle to Charlie, and then buying the green bicycle from Charlie\n"
     ]
    }
   ],
   "source": [
    "# Test with LSH model (first layer only)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RESULT 2: LSH MODEL (First Layer with LSH Attention)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "inputs = tokenizer(test_prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs_lsh = model.generate(\n",
    "        inputs.input_ids,\n",
    "        max_new_tokens=50,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "# Decode only the newly generated tokens (skip the input prompt)\n",
    "input_length = inputs.input_ids.shape[1]\n",
    "response_lsh = tokenizer.decode(outputs_lsh[0][input_length:], skip_special_tokens=True)\n",
    "print(f\"\\nResponse:\")\n",
    "print(response_lsh)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "COMPARISON\n",
      "============================================================\n",
      "\n",
      "Prompt: In a small village, there were three friends: Alice, Bob, and Charlie. Alice owned a red bicycle, Bob had a blue bicycle, and Charlie's bicycle was green. One day, Alice lent her red bicycle to Bob, who then gave his blue bicycle to Charlie. Later that week, Charlie returned the blue bicycle to Bob, and Bob gave the red bicycle back to Alice. \n",
      "\n",
      "The following week, Alice decided to sell her red bicycle to a neighbor named David. Bob, who had always wanted a red bicycle, was disappointed. Charlie, feeling sorry for Bob, offered to sell his green bicycle to Bob for a fair price. Bob agreed and bought the green bicycle from Charlie.\n",
      "\n",
      "A month later, David decided he didn't need the red bicycle anymore and sold it back to Alice. Alice, remembering how much Bob had wanted it, immediately offered to give the red bicycle to Bob as a gift. Bob was thrilled and accepted the gift.\n",
      "\n",
      "Question: At the end of this story, what color bicycle does Bob own, and how did he acquire it?\n",
      "\n",
      "------------------------------------------------------------\n",
      "ORIGINAL (Standard Attention):\n",
      "------------------------------------------------------------\n",
      "Answer: At the end of the story, Bob owns a red bicycle, and he acquired it through a series of transactions involving Alice, who initially lent it to him, and David, who bought it back from her. He also acquired it from Charlie\n",
      "\n",
      "------------------------------------------------------------\n",
      "LSH (First Layer with LSH Attention):\n",
      "------------------------------------------------------------\n",
      "Answer: \n",
      "At the end of the story, Bob owns a red bicycle. He acquired it by first borrowing Alice's red bicycle, then receiving a blue bicycle from Bob, giving the blue bicycle to Charlie, and then buying the green bicycle from Charlie\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Side-by-side comparison\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nPrompt: {test_prompt}\")\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"ORIGINAL (Standard Attention):\")\n",
    "print(\"-\"*60)\n",
    "print(response_original)\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"LSH (First Layer with LSH Attention):\")\n",
    "print(\"-\"*60)\n",
    "print(response_lsh)\n",
    "print(\"\\n\" + \"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Additional Analysis:\n",
      "Original response length: 43 words\n",
      "LSH response length: 42 words\n",
      "\n",
      "Original tokens: 265 tokens\n",
      "LSH tokens: 265 tokens\n"
     ]
    }
   ],
   "source": [
    "# Additional analysis: Compare token probabilities or other metrics\n",
    "print(\"\\nAdditional Analysis:\")\n",
    "print(f\"Original response length: {len(response_original.split())} words\")\n",
    "print(f\"LSH response length: {len(response_lsh.split())} words\")\n",
    "print(f\"\\nOriginal tokens: {len(outputs_original[0])} tokens\")\n",
    "print(f\"LSH tokens: {len(outputs_lsh[0])} tokens\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "### What We Did\n",
    "1. **BitNet Usage**: Standard Hugging Face model, works with `generate()`\n",
    "2. **FlexAttention**: Enables custom attention via `score_mod` function\n",
    "3. **Half-Space LSH**: Simple but effective sparse attention pattern\n",
    "4. **Integration**: Requires wrapping attention modules and intercepting forward pass\n",
    "5. **Comparison**: LSH attention changes model behavior (as expected)\n",
    "\n",
    "### Observations:\n",
    "\n",
    "- LSH attention successfully masks ~50% of attention pairs\n",
    "- The model still generates coherent text (though different from original)\n",
    "- Only the first layer was modified - modifying all layers would have a larger effect\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. **Modify all layers**: Replace attention in all transformer layers\n",
    "2. **Experiment with hyperplanes**: Try multiple hyperplanes or learned hyperplanes\n",
    "3. **Benchmark performance**: Measure speed and memory usage\n",
    "4. **Evaluate quality**: Use metrics like perplexity or task-specific scores\n",
    "5. **Fine-tune**: Adjust hyperplane initialization or masking strategy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
